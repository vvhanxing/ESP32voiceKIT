{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "import vtk\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1060'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = DepthAnythingV2(encoder='vitb', features=128, out_channels=[96, 192, 384, 768])\n",
    "model.load_state_dict(torch.load('./checkpoints/depth_anything_v2_vitb.pth', map_location='cuda:0')) #cpu\n",
    "\n",
    "\n",
    "# model = DepthAnythingV2(encoder='vits', features=64, out_channels=[48, 96, 192, 384])\n",
    "# model.load_state_dict(torch.load('./checkpoints/depth_anything_v2_vits.pth', map_location='cuda:0'))\n",
    "\n",
    "def pic23d(pic_path,save_path):\n",
    "    model.cuda().eval()   #   cpu model.eval() \n",
    "\n",
    "    raw_img = cv2.imread(pic_path)  \n",
    "    height, width = raw_img.shape[:2]  \n",
    "    target_height = 600  \n",
    "    scale_ratio = target_height / height  \n",
    "    target_width = int(width * scale_ratio)  \n",
    "    raw_img = cv2.resize(raw_img, (target_width, target_height))  \n",
    "    \n",
    "    depth = model.infer_image(raw_img) # HxW raw depth map\n",
    "    depth_normalized = cv2.normalize(depth, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)  \n",
    "    depth_display = (depth_normalized * 255).astype(np.uint8)  # 转换为8位图像以便显示  \n",
    "    cv2.imwrite('./depth_map.png', depth_display)  \n",
    "\n",
    "    fx = 50  # 焦距x  \n",
    "    fy = 50# 焦距y  \n",
    "    cx = raw_img.shape[1] / 2.0  # 光心x  \n",
    "    cy = raw_img.shape[0] / 2.0  # 光心y  \n",
    "    height, width = raw_img.shape[:2]  \n",
    "    \n",
    "    # 推断深度图  \n",
    "    depth_ = depth_display/255\n",
    "    # 深度图可能需要归一化或转换到实际的深度值，这里假设它已经是所需的格式  \n",
    "    \n",
    "    # 创建一个空的数组来存储空间位置坐标  \n",
    "    pos = np.zeros((height * width, 3), dtype=np.float32)  \n",
    "    \n",
    "    # 遍历深度图的每个像素  \n",
    "\n",
    "\n",
    "    # 创建一个空的数组来存储空间位置坐标  \n",
    "    pos = np.zeros((height * width, 3), dtype=np.float32)  \n",
    "    \n",
    "    # 遍历深度图的每个像素  \n",
    "    # Calculate indices for each pixel\n",
    "    v_indices,u_indices = np.indices(depth_.shape)\n",
    "\n",
    "    # Calculate spatial coordinates using vectorized operations\n",
    "    X = (u_indices - cx) / fx\n",
    "    Y = (v_indices - cy) / fy\n",
    "    Z = depth_ * 10\n",
    "\n",
    "    # Stack the coordinates into a single array\n",
    "    pos = np.stack((X, Y, Z), axis=-1).reshape(-1, 3)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # for v in range(height):  \n",
    "    #     for u in range(width):  \n",
    "    #         d = depth_[v, u]  # 获取深度值  \n",
    "    #         # 计算空间坐标  \n",
    "    #         X = (u - cx) * 1/ fx  \n",
    "    #         Y = (v - cy)  * 1/ fy  \n",
    "    #         Z = d  *10\n",
    "    #         # 将坐标存储到pos数组中  \n",
    "    #         idx = v * width + u  \n",
    "    #         pos[idx] = [X, Y, Z]  \n",
    "    \n",
    "    # 现在pos数组包含了深度图上每个点的空间位置坐标\n",
    "\n",
    "\n",
    "    with open(\"a.obj\",\"w\") as t:\n",
    "        t.writelines(\"mtllib my_mtl.mtl\"+\"\\n\")\n",
    "            \n",
    "        for l in pos:\n",
    "            L = \"v \"+str(l[0])+\" \"+str(l[1])+\" \"+str(l[2])+\"\\n\"\n",
    "            t.writelines(L )\n",
    "        \n",
    "        w_x = np.asarray([np.linspace(0.0, 1.0, num=width)]*height).reshape(width*height)\n",
    "        h_y = np.asarray([np.linspace(1.0, 0.0, num=height)]*width).T.reshape(width*height)\n",
    "        t_array_ = np.stack([w_x,h_y],axis=1)\n",
    "\n",
    "\n",
    "        for i,j in t_array_:\n",
    "            L = \"vt \" +str(i)+\" \"+str(j) +\"\\n\"\n",
    "            t.writelines(L )                \n",
    "    #  \n",
    "    #          \n",
    "        t.writelines(\"\\nusemtl my_mtl\\n\" )              \n",
    "        b = np.array(range(height * width)).reshape([ height ,width])\n",
    "        print(b.shape)  \n",
    "                \n",
    "        for i in range(1,b.shape[0]-1):\n",
    "            for j in range(1,b.shape[1]-1):\n",
    "                p = [ b[i,j] , b[i+1,j],b[i+1,j+1],b[i,j+1]]\n",
    "        \n",
    "                L = \"f \"+str(p[0])+\"/\"+ str(p[0])+\" \"+str(p[1])+\"/\"+ str(p[1])+\" \"+str(p[2])+\"/\"+ str(p[2])+\" \"+str(p[3])+\"/\"+ str(p[3])+\"\\n\"\n",
    "                t.writelines(L )\n",
    "            \n",
    "\n",
    "\n",
    "    with open(\"my_mtl.mtl\",\"w\") as m:  \n",
    "        m.writelines(\n",
    "    f\"\"\"\n",
    "    newmtl my_mtl\n",
    "    Ka 1 1 1\n",
    "    Kd 1 1 1\n",
    "    d 1\n",
    "    Ns 0\n",
    "    illum 1\n",
    "    map_Kd {pic_path}\n",
    "    \"\"\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def render_and_save_images(obj_file,  output_file, image_size=(800, 800)):\n",
    "        # 读取模型\n",
    "        reader = vtk.vtkOBJReader()\n",
    "        reader.SetFileName(obj_file)\n",
    "        reader.Update()\n",
    "\n",
    "        # 创建一个映射器\n",
    "        mapper = vtk.vtkPolyDataMapper()\n",
    "        mapper.SetInputConnection(reader.GetOutputPort())\n",
    "\n",
    "        # 创建一个演员\n",
    "        actor = vtk.vtkActor()\n",
    "        actor.SetMapper(mapper)\n",
    "\n",
    "        # 读取贴图文件\n",
    "        texture = vtk.vtkTexture()\n",
    "        texture_reader = vtk.vtkJPEGReader()  # 假设贴图为JPEG格式\n",
    "        texture_reader.SetFileName(pic_path)  # 根据.mtl文件中的路径设置\n",
    "        texture_reader.Update()\n",
    "        texture.SetInputConnection(texture_reader.GetOutputPort())\n",
    "        \n",
    "        # 将贴图应用到演员\n",
    "        actor.SetTexture(texture)\n",
    "        actor.SetOrientation(20,0,0)\n",
    "\n",
    "        # 创建渲染窗口\n",
    "        render_window = vtk.vtkRenderWindow()\n",
    "        render_window.SetSize(image_size[0], image_size[1])  # 设置渲染窗口大小 \n",
    "\n",
    "        # 创建左右视角的渲染器\n",
    "        total_pic_count = 36\n",
    "        renderers = []\n",
    "        for i in range(total_pic_count):\n",
    "            renderer = vtk.vtkRenderer()\n",
    "            render_window.AddRenderer(renderer)\n",
    "            renderers.append(renderer)\n",
    "            # 禁用光照\n",
    "            renderer.SetAmbient(100.0, 100.0, 100.0)  # 设置环境光\n",
    "            actor.GetProperty().SetLighting(False)\n",
    "            # 设置摄像机\n",
    "            camera = vtk.vtkCamera()\n",
    "            \n",
    "            camera.SetFocalPoint(0, 0, 0)\n",
    "            # if i == 0:  # 左视角\n",
    "            #     camera.SetPosition(-2, 0, 30)  # 可以根据需要调整位置\n",
    "            #     camera.SetViewUp(0, -1, 0)\n",
    "            # else:  # 右视角\n",
    "            #     camera.SetPosition(2, 0, 30)  # 右视角位置\n",
    "            #     camera.SetViewUp(0, -1, 0)\n",
    "\n",
    "      \n",
    "            camera.SetPosition(-9+i*0.5, 0, 20)  # 可以根据需要调整位置\n",
    "            camera.SetViewUp(0, -1, 0)\n",
    "\n",
    "            renderer.SetActiveCamera(camera)\n",
    "\n",
    "            # 添加演员到渲染器\n",
    "            renderer.AddActor(actor)\n",
    "            renderer.SetBackground(0.0, 0.0, 0.0)  # 背景颜色\n",
    "\n",
    "            # 渲染\n",
    "            render_window.Render()\n",
    "\n",
    "            # 保存图像\n",
    "            window_to_image_filter = vtk.vtkWindowToImageFilter()\n",
    "            window_to_image_filter.SetInput(render_window)\n",
    "            window_to_image_filter.ReadFrontBufferOff()  # 读取后台缓冲区\n",
    "            window_to_image_filter.Update()\n",
    "\n",
    "            writer = vtk.vtkPNGWriter()\n",
    "            writer.SetFileName(save_path.replace(\"_/\",f\"{i+1}/\"))\n",
    "        \n",
    "            writer.SetInputConnection(window_to_image_filter.GetOutputPort())\n",
    "            writer.Write()\n",
    "\n",
    "    \n",
    "\n",
    "    render_and_save_images('a.obj',  'output.png')\n",
    "\n",
    "\n",
    "# pic_path = \"./37.jpg\"\n",
    "# save_path = \"./1.png\"\n",
    "# pic23d(pic_path,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 337)\n",
      "(600, 337)\n",
      "已保存 250 帧到 ./blender_gif/\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def save_frames_from_video(video_path, output_folder, num_frames_range=[100,102]):\n",
    "    # 创建输出文件夹\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 打开视频文件\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # 检查视频是否成功打开\n",
    "    if not cap.isOpened():\n",
    "        print(\"无法打开视频文件\")\n",
    "        return\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "\n",
    "    for i in range(0,36):\n",
    "        save_path = f\"{output_folder}/{i+1}/\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    #while (frame_count >= num_frames_range[0] and frame_count < num_frames_range[1]) :\n",
    "    # for i in range(num_frames_range[0],num_frames_range[1]):\n",
    "    pic_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # 读取帧\n",
    "        if not ret:\n",
    "            break  # 如果没有更多帧，退出循环\n",
    "        frame_count += 1\n",
    "        if  (frame_count >= num_frames_range[0] and frame_count < num_frames_range[1]) :\n",
    "            pic_count+=1\n",
    "\n",
    "            # 保存帧为图像文件\n",
    "            frame_filename = os.path.join(output_folder, f'{pic_count}.jpg')\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            \n",
    "\n",
    "            save_path = frame_filename.replace(\"blender_gif/\",\"blender_gif/_/\")\n",
    "\n",
    "            pic23d(frame_filename,save_path)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    cap.release()  # 释放视频捕获对象\n",
    "    print(f'已保存 {frame_count} 帧到 {output_folder}')\n",
    "\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "video_file = './video.mp4'  # 视频文件路径\n",
    "output_dir = './blender_gif/'    # 输出文件夹路径\n",
    "save_frames_from_video(video_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def upload_pic(pic_file_name, save_base=\"index.bin\"):\n",
    "    im = Image.open(pic_file_name)\n",
    "    \n",
    "    # Adjust the image size while maintaining the aspect ratio\n",
    "    original_width, original_height = im.size\n",
    "    target_height = 240\n",
    "    target_width = int((target_height / original_height) * original_width)\n",
    "    im = im.resize((target_width, target_height))  # Use LANCZOS filter for better quality\n",
    "\n",
    "    # Optionally, convert to RGB if the image has an alpha channel\n",
    "    if im.mode in (\"RGBA\", \"LA\") or (im.mode == \"P\" and \"transparency\" in im.info):\n",
    "        im = im.convert(\"RGB\")\n",
    "\n",
    "    # Save as a binary file (using JPEG with adjustable quality)\n",
    "    img_byte = BytesIO()\n",
    "    im.save(img_byte, format='JPEG', quality=100)  # Adjust quality as needed\n",
    "    img_data = img_byte.getvalue()\n",
    "\n",
    "    # Write to binary file\n",
    "    with open(save_base, \"wb\") as f:\n",
    "        f.write(img_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pic_path = \"./blender_gif/\"\n",
    "\n",
    " \n",
    "for i in range(1,37):\n",
    "    for pic in os.listdir(pic_path+\"/\"+str(i)):\n",
    "        if \".jpg\" in pic or \".png\" in pic:\n",
    "            # print(pic_path+pic)\n",
    "            if not os.path.exists(\"D:/move2/\"+str(i)+\"/\"):\n",
    "                os.mkdir(\"D:/move2/\"+str(i)+\"/\")\n",
    "            upload_pic(pic_path+\"/\"+str(i)+\"/\"+pic,\"D:/move2/\"+str(i)+\"/\"+str(int(pic.split(\".\")[0]))+\".bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vv\\Miniconda3\\envs\\llama2\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "from perspective2d import PerspectiveFields as _PerspectiveFields\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "from perspective2d import PerspectiveFields as _PerspectiveFields\n",
    "\n",
    "\n",
    "class PerspectiveFields(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Thin wrapper around perspective fields model for inferring camera intrinsics\n",
    "    \"\"\"\n",
    "    VERSIONS = {\n",
    "        \"Paramnet-360Cities-edina-centered\",\n",
    "        \"Paramnet-360Cities-edina-uncentered\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            version=\"Paramnet-360Cities-edina-uncentered\",\n",
    "            #\n",
    "            #Paramnet-360Cities-edina-centered\n",
    "            device=\"cuda\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            version (str): underlying model version to use. Valid options are:\n",
    "                {\"Paramnet-360Cities-edina-uncentered\", \"Paramnet-360Cities-edina-centered\"}\n",
    "            device (str): device to store tensors on. Default is \"cuda\"\n",
    "        \"\"\"\n",
    "        # Call super first\n",
    "        super().__init__()\n",
    "\n",
    "        # Sanity check version\n",
    "        assert version in self.VERSIONS,\\\n",
    "            f\"Got invalid PerspectiveFields version! Valid options: {self.VERSIONS}, got: {version}\"\n",
    "\n",
    "        # Load model\n",
    "        self.device = device\n",
    "        self.model = _PerspectiveFields(version)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def estimate_camera_intrinsics(self, input_path):\n",
    "        \"\"\"\n",
    "        Estimates the K camera intrinsics matrix using PerspectiveFields model\n",
    "\n",
    "        Args:\n",
    "            input_path (str): Absolute path to the image from which to infer camera intrinsics\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Estimated intrinsics matrix\n",
    "        \"\"\"\n",
    "        # Load the image\n",
    "        img_bgr = cv2.imread(input_path)\n",
    "\n",
    "        # Run predictions\n",
    "        predictions = self.model.inference(img_bgr=img_bgr)\n",
    "\n",
    "        # Compute intrinsics\n",
    "        height, width, _ = img_bgr.shape\n",
    "        # Convert vfov to radians\n",
    "        vfov_rad = math.radians(predictions['pred_general_vfov'].item())\n",
    "        # Compute focal length in pixels\n",
    "        f = (height / 2) / math.tan(vfov_rad / 2)\n",
    "        # Compute fx and fy\n",
    "        fx = f * predictions['pred_rel_focal'].item()\n",
    "        fy = f\n",
    "        # Compute cx and cy\n",
    "        cx = width / 2.0\n",
    "        cy = height / 2.0\n",
    "        # Construct the camera intrinsics matrix\n",
    "        K = torch.tensor([\n",
    "            [fx, 0, cx],\n",
    "            [0, fy, cy],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        print(K)\n",
    "\n",
    "        return K\n",
    "    \n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# import digital_cousins\n",
    "from metric_depth.depth_anything_v2.dpt import DepthAnythingV2 as _DepthAnythingV2\n",
    "\n",
    "# from digital_cousins.utils.processing_utils import process_depth_linear, unprocess_depth_linear\n",
    "\n",
    "def rescale_image(img, in_limits, out_limits):\n",
    "    \"\"\"\n",
    "    Rescales image from having values in range @in_limits to range @out_limits\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Image to be rescaled\n",
    "        in_limits (2-tuple): (min, max) range of the input image\n",
    "        out_limits (2-tuple): (min, max) range of the rescaled image\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Rescaled image\n",
    "    \"\"\"\n",
    "    # Out shape if specified should be (H, W) 2-tuple\n",
    "    # Keep absolute range to be from 0 to 10 --> renormalize to 0 - 1\n",
    "    in_min, in_max = in_limits\n",
    "    out_min, out_max = out_limits\n",
    "    scale_factor = abs(out_max - out_min) / abs(in_max - in_min)\n",
    "    out_tf = (out_max + out_min) / 2.0\n",
    "    in_tf = (in_max + in_min) / 2.0\n",
    "    img = (img - in_tf) * scale_factor + out_tf\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def process_depth_linear(depth, in_limits=(0, 10), out_limits=(0, 1), out_shape=None, use_16bit=True):\n",
    "    \"\"\"\n",
    "    Discretizes a linear depth map from range @in_limits to range @out_limits, and optionally resizes it to @out_shape\n",
    "\n",
    "    Args:\n",
    "        depth (np.ndarray): Input depth map with metric values\n",
    "        in_limits (2-tuple): (min, max) range of the input image\n",
    "        out_limits (2-tuple): (min, max) range of the processed image\n",
    "        out_shape (None or 2-tuple): If specified, the (H, W) @depth should be resized to\n",
    "        use_16bit (bool): Whether to use 16-bit or 8-bit uint encoding when rescaling the image\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Processed depth image.\n",
    "    \"\"\"\n",
    "    # Out shape if specified should be (H, W) 2-tuple\n",
    "    # Keep absolute range to be from 0 to 10 --> renormalize to 0 - 1\n",
    "    in_min, in_max = in_limits\n",
    "    foreground = np.where(depth <= in_max)\n",
    "    background = np.where(depth > in_max)\n",
    "    depth = rescale_image(img=depth, in_limits=in_limits, out_limits=out_limits)\n",
    "\n",
    "    # Zero out background\n",
    "    bit_size = 16 if use_16bit else 8\n",
    "    dtype = np.uint16 if use_16bit else np.uint8\n",
    "    depth[background] = 0.0\n",
    "    # Multiply by 2 ** 16\n",
    "    depth = depth * (2 ** bit_size)\n",
    "    depth = depth.astype(dtype)\n",
    "    if out_shape is not None:\n",
    "        depth = cv2.resize(depth, (out_shape[1], out_shape[0]))\n",
    "    return depth\n",
    "\n",
    "\n",
    "def unprocess_depth_linear(depth, in_limits=(0.0, 1.0), out_limits=(0.0, 10.0)):\n",
    "    \"\"\"\n",
    "    Unnormalizes a linear depth map from range @in_limits to range @out_limits. This process is the inverse\n",
    "    of @procedss_depth_linear)\n",
    "\n",
    "    Args:\n",
    "        depth (np.ndarray): Input depth map with normalized values (the output of @process_depth_linear)\n",
    "        in_limits (2-tuple): (min, max) range of the input image\n",
    "        out_limits (2-tuple): (min, max) range of the unprocessed image\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Unnormalized depth image.\n",
    "    \"\"\"\n",
    "    # Map to float, divide by 2**16, then invert transform\n",
    "    depth = depth.astype(float)\n",
    "    depth = depth / (2 ** 16)\n",
    "\n",
    "    # Keep unnormalize from 0 - 1 --> 0 - 10\n",
    "    in_min, in_max = in_limits\n",
    "    out_min, out_max = out_limits\n",
    "    scale_factor = abs(out_max - out_min) / abs(in_max - in_min)\n",
    "    out_tf = (out_max + out_min) / 2.0\n",
    "    in_tf = (in_max + in_min) / 2.0\n",
    "    depth = (depth - in_tf) * scale_factor + out_tf\n",
    "    return depth\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DepthAnythingV2(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Thin wrapper around DepthAnything V2 model for inferring depth maps\n",
    "    \"\"\"\n",
    "    # Maps backbone size to configuration to use\n",
    "    CONFIGS = {\n",
    "        'small': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "        'base': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "        'large': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    }\n",
    "\n",
    "    # Maps dataset name to max depth to use\n",
    "    DATASET_MAX_DEPTHS = {\n",
    "        \"hypersim\": 20,\n",
    "        \"vkitti\": 80,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone_size=\"base\",\n",
    "            backbone_dataset=\"hypersim\",\n",
    "            max_depth=None,\n",
    "            device=\"cuda\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            backbone_size (str): Size of underlying ViT model to use. Options are {'small', 'base', 'large'}\n",
    "            backbone_dataset (str): Which training dataset the underlying model was trained on.\n",
    "                Use \"hypersim\" for indoor estimation, \"vkitti\" for outdoor model\n",
    "            max_depth (None or int): Maximum depth (m) to use for depth estimation. If None, will use a default\n",
    "                based on @backbone_dataset\n",
    "            device (str): device to store tensors on. Default is \"cuda\"\n",
    "        \"\"\"\n",
    "        # Call super first\n",
    "        super().__init__()\n",
    "\n",
    "        # Sanity check values\n",
    "        assert backbone_size in self.CONFIGS,\\\n",
    "            f\"Got invalid DepthAnythingV2 backbone_size! Valid options: {self.CONFIGS.keys()}, got: {backbone_size}\"\n",
    "        assert backbone_dataset in self.DATASET_MAX_DEPTHS,\\\n",
    "            f\"Got invalid DepthAnythingV2 backbone_dataset! Valid options: {self.DATASET_MAX_DEPTHS.keys()}, got: {backbone_dataset}\"\n",
    "\n",
    "        self.max_depth = self.DATASET_MAX_DEPTHS[backbone_dataset] if max_depth is None else max_depth\n",
    "        self.device = 'cuda:0'\n",
    "\n",
    "        #Load model\n",
    "        self.model = _DepthAnythingV2(**{**self.CONFIGS[backbone_size], \"max_depth\": self.max_depth})\n",
    "        self.model.load_state_dict(\n",
    "            torch.load(\n",
    "                './checkpoints/depth_anything_v2_metric_hypersim_vitb.pth',\n",
    "                map_location=self.device,\n",
    "                weights_only=True,\n",
    "            ),\n",
    "        )\n",
    "        self.model.to(self.device).eval()\n",
    "\n",
    "        # self. model = _DepthAnythingV2(encoder='vitb', features=128, out_channels=[96, 192, 384, 768])\n",
    "        # self.model.load_state_dict(torch.load('./checkpoints/depth_anything_v2_vitb.pth', map_location='cuda:0')) #cpu\n",
    "        # self.model.cuda().eval() \n",
    "        #depth = model.infer_image(raw_img) # HxW raw depth map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def estimate_depth_linear(self, input_path, output_path, depth_limits=(0, 10.0)):\n",
    "        \"\"\"\n",
    "        Estimates linear depth map using DepthAnythingV2 model.\n",
    "\n",
    "        NOTE: depth linear implies that the outputted map is taken with respect to the image plane,\n",
    "            NOT the camera position!\n",
    "\n",
    "        Args:\n",
    "            input_path (str): Absolute path to the image from which to infer linear depth map\n",
    "            output_path (str): Absolute path to the location for the estimated linear depth map image\n",
    "            depth_limits (2-tuple): (min, max) values from the depth map to normalize scaling when saving the\n",
    "                depth map image\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Estimated linear depth map\n",
    "        \"\"\"\n",
    "        img = cv2.imread(input_path)\n",
    "        H, W, _ = img.shape\n",
    "        pred = self.model.infer_image(img)  # HxW depth map in meters in numpy\n",
    "\n",
    "        # Save output; normalize and convert the depth image to an 8-bit format if necessary\n",
    "        Path(os.path.dirname(output_path)).mkdir(parents=True, exist_ok=True)\n",
    "        depth = process_depth_linear(depth=pred, in_limits=depth_limits)\n",
    "        Image.fromarray(depth).save(output_path)\n",
    "\n",
    "        return depth\n",
    "\n",
    "    def crop_center(self, image, crop_percent=1):\n",
    "        \"\"\"\n",
    "        Crops the center of the image based on the specified crop percentage.\n",
    "        Works for both single-channel (grayscale/depth) and three-channel (RGB) images.\n",
    "\n",
    "        Params:\n",
    "            image (np.ndarray): A numpy array representing the image. Can be 2D (single-channel) or 3D (multi-channel).\n",
    "            crop_percent (int): The percentage of the image to retain in the center. Default is 0.8.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: A numpy array representing the cropped image.\n",
    "        \"\"\"\n",
    "\n",
    "        if crop_percent <= 0 or crop_percent > 1:\n",
    "            raise ValueError(\"crop_percent must be between 0 and 1\")\n",
    "\n",
    "        # Check the number of dimensions to handle both single and multi-channel images\n",
    "        if image.ndim == 3:  # Multi-channel image\n",
    "            height, width, _ = image.shape\n",
    "        elif image.ndim == 2:  # Single-channel image\n",
    "            height, width = image.shape\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image shape\")\n",
    "\n",
    "        # Calculate the new dimensions\n",
    "        new_height = int(height * crop_percent)\n",
    "        new_width = int(width * crop_percent)\n",
    "\n",
    "        # Calculate margins\n",
    "        top_margin = (height - new_height) // 2\n",
    "        left_margin = (width - new_width) // 2\n",
    "\n",
    "        # Crop the image based on the number of dimensions\n",
    "        if image.ndim == 3:\n",
    "            cropped_image = image[top_margin:top_margin + new_height, left_margin:left_margin + new_width, :]\n",
    "        else:  # image.ndim == 2\n",
    "            cropped_image = image[top_margin:top_margin + new_height, left_margin:left_margin + new_width]\n",
    "\n",
    "        print(f\"cropped_img_shape: {cropped_image.shape}\")\n",
    "        return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9464e+03, 0.0000e+00, 1.6850e+02],\n",
      "        [0.0000e+00, 1.0859e+03, 3.0000e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "depth_max_limit = 20\n",
    "\n",
    "def compute_point_cloud_from_depth(depth, K, cam_to_img_tf=None, world_to_cam_tf=None, visualize_every=0,\n",
    "                                   grid_limits=None):\n",
    "    \"\"\"\n",
    "    Computes point cloud from depth image.\n",
    "\n",
    "    Args:\n",
    "        depth (np.ndarray): Input depth map with normalized values (the output of @process_depth_linear)\n",
    "        K (np.ndarray): 3x3 cam intrinsics matrix\n",
    "        cam_to_img_tf (np.ndarray): 4x4 Camera to image coordinate transformation matrix.\n",
    "                    omni cam_to_img_tf is T.pose2mat(([0, 0, 0], T.euler2quat([np.pi, 0, 0])))\n",
    "        world_to_cam_tf (np.ndarray): 4x4 World to camera coordinate transformation matrix.\n",
    "        visualize_every (int): Step size when uniformly sampling points in the resulting point cloud to visualize.\n",
    "        grid_limits (float): Visualization plot grid limits.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Resulting point cloud.\n",
    "    \"\"\"\n",
    "    \n",
    "    h, w = depth.shape\n",
    "    y, x = np.meshgrid(np.arange(h), np.arange(w), indexing=\"ij\", sparse=False)\n",
    "    assert depth.min() >= 0\n",
    "    u = x\n",
    "    v = y\n",
    "    uv = np.dstack((u, v, np.ones_like(u)))\n",
    "\n",
    "    Kinv = np.linalg.inv(K)\n",
    "\n",
    "    pc = depth.reshape(-1, 1) * (uv.reshape(-1, 3) @ Kinv.T)\n",
    "    pc = pc.reshape(h, w, 3)\n",
    "\n",
    "    # import open3d as o3d\n",
    "    # pcd = o3d.geometry.PointCloud()\n",
    "    # pcd.points = o3d.utility.Vector3dVector(pc.reshape(-1, 3))\n",
    "    # # pcd.colors = o3d.utility.Vector3dVector(rgb.reshape(-1, 3) / 255.0)\n",
    "    # o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    # If no tfs, use identity matrix\n",
    "    cam_to_img_tf = np.eye(4) if cam_to_img_tf is None else cam_to_img_tf\n",
    "    world_to_cam_tf = np.eye(4) if world_to_cam_tf is None else world_to_cam_tf\n",
    "\n",
    "    pc = np.concatenate([pc.reshape(-1, 3), np.ones((h * w, 1))], axis=-1)  # shape (H*W, 4)\n",
    "\n",
    "    # Convert using camera transform\n",
    "    # Create (H * W, 4) vector from pc\n",
    "    pc = (pc @ cam_to_img_tf.T @ world_to_cam_tf.T)[:, :3].reshape(h, w, 3)\n",
    "    return pc,h,w\n",
    "device = \"cuda:0\"\n",
    "input_path = \"./1.jpg\"\n",
    "save_dir = \".\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "raw_img = cv2.imread(input_path)  \n",
    "height, width = raw_img.shape[:2]  \n",
    "target_height = 600  \n",
    "scale_ratio = target_height / height  \n",
    "target_width = int(width * scale_ratio)  \n",
    "raw_img = cv2.resize(raw_img, (target_width, target_height))  \n",
    "cv2.imwrite(input_path, raw_img)\n",
    "\n",
    "camera_intrinsics_matrix = None\n",
    "if camera_intrinsics_matrix is None:\n",
    "   \n",
    "    intrinsics_estimator = PerspectiveFields(device=device)\n",
    "    intrinsics = intrinsics_estimator.estimate_camera_intrinsics(input_path=input_path)\n",
    "    intrinsics_estimator.to('cpu')\n",
    "    del intrinsics_estimator\n",
    "    camera_intrinsics_matrix = np.array(intrinsics)\n",
    "\n",
    "\n",
    "depth_path = f\"{save_dir}/step_1_depth.png\"\n",
    "depth_limits = np.array([0, depth_max_limit])\n",
    "depth_estimator = DepthAnythingV2(device=device)\n",
    "\n",
    "\n",
    "\n",
    "depth_estimator.estimate_depth_linear(input_path=input_path, output_path=depth_path, depth_limits=depth_limits)\n",
    "depth_estimator.to('cpu')\n",
    "del depth_estimator\n",
    "\n",
    "depth = unprocess_depth_linear(np.array(Image.open(depth_path)), out_limits=depth_limits)\n",
    "pc = compute_point_cloud_from_depth(depth=depth, K=camera_intrinsics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "visualize = True\n",
    "if visualize:\n",
    "    # Explicitly manage visualization process to prevent conflict with og launching process\n",
    "    def vis():\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(pc[0].reshape(-1, 3))\n",
    "        # pcd.colors = o3d.utility.Vector3dVector(rgb.reshape(-1, 3) / 255.0)\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "    # vis_process = multiprocessing.Process(target=vis)\n",
    "    # vis_process.start()\n",
    "    # vis_process.join()\n",
    "vis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v -5.718049773986422 -8.629110509991733 19.99969482421875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in pc[0]:\n",
    "    print( \"v \"+str(l[0][0])+\" \"+str(l[0][1])+\" \"+str(l[0][2])+\"\\n\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202200, 3)\n",
      "(600, 337)\n"
     ]
    }
   ],
   "source": [
    "pos ,height ,width= pc  #pc,h,w\n",
    "pos = pos.reshape(-1, 3)\n",
    "print(pos.shape)\n",
    "with open(\"a.obj\",\"w\") as t:\n",
    "    t.writelines(\"mtllib my_mtl.mtl\"+\"\\n\")\n",
    "        \n",
    "    for l in pos:\n",
    "        L =  \"v \"+str(l[0])+\" \"+str(l[1])+\" \"+str(l[2])+\"\\n\"\n",
    "        t.writelines(L )\n",
    "    \n",
    "    w_x = np.asarray([np.linspace(0.0, 1.0, num=width)]*height).reshape(width*height)\n",
    "    h_y = np.asarray([np.linspace(1.0, 0.0, num=height)]*width).T.reshape(width*height)\n",
    "    t_array_ = np.stack([w_x,h_y],axis=1)\n",
    "\n",
    "\n",
    "    for i,j in t_array_:\n",
    "        L = \"vt \" +str(i)+\" \"+str(j) +\"\\n\"\n",
    "        t.writelines(L )                \n",
    "#  \n",
    "#          \n",
    "    t.writelines(\"\\nusemtl my_mtl\\n\" )              \n",
    "    b = np.array(range(height * width)).reshape([ height ,width])\n",
    "    print(b.shape)  \n",
    "            \n",
    "    for i in range(1,b.shape[0]-1):\n",
    "        for j in range(1,b.shape[1]-1):\n",
    "            p = [ b[i,j] , b[i+1,j],b[i+1,j+1],b[i,j+1]]\n",
    "    \n",
    "            L = \"f \"+str(p[0])+\"/\"+ str(p[0])+\" \"+str(p[1])+\"/\"+ str(p[1])+\" \"+str(p[2])+\"/\"+ str(p[2])+\" \"+str(p[3])+\"/\"+ str(p[3])+\"\\n\"\n",
    "            t.writelines(L )\n",
    "        \n",
    "\n",
    "\n",
    "with open(\"my_mtl.mtl\",\"w\") as m:  \n",
    "    m.writelines(\n",
    "f\"\"\"\n",
    "newmtl my_mtl\n",
    "Ka 1 1 1\n",
    "Kd 1 1 1\n",
    "d 1\n",
    "Ns 0\n",
    "illum 1\n",
    "map_Kd {input_path}\n",
    "\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************\n",
      "Starting tests...\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vv\\Miniconda3\\envs\\llama2\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      ".//checkpoints\n",
      "['depth_anything_v2_metric_hypersim_vitb.pth', 'depth_anything_v2_vitb.pth', 'depth_anything_v2_vits.pth']\n"
     ]
    }
   ],
   "source": [
    "import digital_cousins\n",
    "# If you store the offline dataset elsewhere, please uncomment the following line and put the directory here\n",
    "# digital_cousins.ASSET_DIR = \"~/assets\"\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "# import digital_cousins\n",
    "\n",
    "TEST_DIR = os.path.dirname(\"test_img.png\")\n",
    "SAVE_DIR = f\"{TEST_DIR}/test_acdc_output\"\n",
    "TEST_IMG_PATH = f\"{TEST_DIR}/test_img.png\"\n",
    "CAPTION = \"Fridge. Cabinet.\"\n",
    "\n",
    "\n",
    "def test_perspective_fields(TEST_IMG_PATH):\n",
    "    from digital_cousins.models.perspective_fields import PerspectiveFields\n",
    "    PerspectiveFields().estimate_camera_intrinsics(input_path=TEST_IMG_PATH)\n",
    "\n",
    "\n",
    "def test_depth_anything_2(TEST_IMG_PATH):\n",
    "    from digital_cousins.models.depth_anything_v2 import DepthAnythingV2\n",
    "    depth_path = f\"test_depth.png\"\n",
    "    DepthAnythingV2().estimate_depth_linear(input_path=TEST_IMG_PATH, output_path=depth_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Run all tests\n",
    "    print()\n",
    "    print(\"*\" * 30)\n",
    "    print(\"Starting tests...\")\n",
    "    print(\"*\" * 30)\n",
    "    print()\n",
    "    TEST_IMG_PATH = \"./test_img.png\"\n",
    "  \n",
    "    test_perspective_fields(TEST_IMG_PATH)\n",
    "    test_depth_anything_2(TEST_IMG_PATH)\n",
    " \n",
    "main()\n",
    "\n",
    "    # Final test -- OG should always come at the end\n",
    "    # This og test cannot run together with test_acdc_step_3\n",
    "    # because the simulator can only be launched once, and after calling og.shutdown(), the whole process will terminate\n",
    "    # test_og(args)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define args\n",
    "#     parser = argparse.ArgumentParser()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
