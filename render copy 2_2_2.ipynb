{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "import vtk\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = DepthAnythingV2(encoder='vitb', features=128, out_channels=[96, 192, 384, 768])\n",
    "model.load_state_dict(torch.load('./checkpoints/depth_anything_v2_vitb.pth', map_location='cuda:0')) #cpu\n",
    "\n",
    "\n",
    "# model = DepthAnythingV2(encoder='vits', features=64, out_channels=[48, 96, 192, 384])\n",
    "# model.load_state_dict(torch.load('./checkpoints/depth_anything_v2_vits.pth', map_location='cuda:0'))\n",
    "\n",
    "def pic23d(pic_path,save_path):\n",
    "    model.cuda().eval()   #   cpu model.eval() \n",
    "\n",
    "    raw_img = cv2.imread(pic_path)  \n",
    "    height, width = raw_img.shape[:2]  \n",
    "    target_height = 600  \n",
    "    scale_ratio = target_height / height  \n",
    "    target_width = int(width * scale_ratio)  \n",
    "    raw_img = cv2.resize(raw_img, (target_width, target_height))  \n",
    "    \n",
    "    depth = model.infer_image(raw_img) # HxW raw depth map\n",
    "    depth_normalized = cv2.normalize(depth, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)  \n",
    "    depth_display = (depth_normalized * 255).astype(np.uint8)  # 转换为8位图像以便显示  \n",
    "    cv2.imwrite('./depth_map.png', depth_display)  \n",
    "\n",
    "    fx = 50  # 焦距x  \n",
    "    fy = 50# 焦距y  \n",
    "    cx = raw_img.shape[1] / 2.0  # 光心x  \n",
    "    cy = raw_img.shape[0] / 2.0  # 光心y  \n",
    "    height, width = raw_img.shape[:2]  \n",
    "    \n",
    "    # 推断深度图  \n",
    "    depth_ = depth_display/255\n",
    "    # 深度图可能需要归一化或转换到实际的深度值，这里假设它已经是所需的格式  \n",
    "    \n",
    "    # 创建一个空的数组来存储空间位置坐标  \n",
    "    pos = np.zeros((height * width, 3), dtype=np.float32)  \n",
    "    \n",
    "    # 遍历深度图的每个像素  \n",
    "\n",
    "\n",
    "    # 创建一个空的数组来存储空间位置坐标  \n",
    "    pos = np.zeros((height * width, 3), dtype=np.float32)  \n",
    "    \n",
    "    # 遍历深度图的每个像素  \n",
    "    # Calculate indices for each pixel\n",
    "    v_indices,u_indices = np.indices(depth_.shape)\n",
    "\n",
    "    # Calculate spatial coordinates using vectorized operations\n",
    "    X = (u_indices - cx) / fx\n",
    "    Y = (v_indices - cy) / fy\n",
    "    Z = depth_ * 10\n",
    "\n",
    "    # Stack the coordinates into a single array\n",
    "    pos = np.stack((X, Y, Z), axis=-1).reshape(-1, 3)\n",
    "    \n",
    "    with open(\"a.obj\",\"w\") as t:\n",
    "        t.writelines(\"mtllib my_mtl.mtl\"+\"\\n\")\n",
    "            \n",
    "        for l in pos:\n",
    "            L = \"v \"+str(l[0])+\" \"+str(l[1])+\" \"+str(l[2])+\"\\n\"\n",
    "            t.writelines(L )\n",
    "        \n",
    "        w_x = np.asarray([np.linspace(0.0, 1.0, num=width)]*height).reshape(width*height)\n",
    "        h_y = np.asarray([np.linspace(1.0, 0.0, num=height)]*width).T.reshape(width*height)\n",
    "        t_array_ = np.stack([w_x,h_y],axis=1)\n",
    "\n",
    "\n",
    "        for i,j in t_array_:\n",
    "            L = \"vt \" +str(i)+\" \"+str(j) +\"\\n\"\n",
    "            t.writelines(L )                \n",
    "    #  \n",
    "    #          \n",
    "        t.writelines(\"\\nusemtl my_mtl\\n\" )              \n",
    "        b = np.array(range(height * width)).reshape([ height ,width])\n",
    "        print(b.shape)  \n",
    "                \n",
    "        for i in range(1,b.shape[0]-1):\n",
    "            for j in range(1,b.shape[1]-1):\n",
    "                p = [ b[i,j] , b[i+1,j],b[i+1,j+1],b[i,j+1]]\n",
    "        \n",
    "                L = \"f \"+str(p[0])+\"/\"+ str(p[0])+\" \"+str(p[1])+\"/\"+ str(p[1])+\" \"+str(p[2])+\"/\"+ str(p[2])+\" \"+str(p[3])+\"/\"+ str(p[3])+\"\\n\"\n",
    "                t.writelines(L )\n",
    "            \n",
    "\n",
    "\n",
    "    with open(\"my_mtl.mtl\",\"w\") as m:  \n",
    "        m.writelines(\n",
    "    f\"\"\"\n",
    "    newmtl my_mtl\n",
    "    Ka 1 1 1\n",
    "    Kd 1 1 1\n",
    "    d 1\n",
    "    Ns 0\n",
    "    illum 1\n",
    "    map_Kd {pic_path}\n",
    "    \"\"\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def render_and_save_images(obj_file,  video = True, image_size=(800, 800)):\n",
    "        # 读取模型\n",
    "        reader = vtk.vtkOBJReader()\n",
    "        reader.SetFileName(obj_file)\n",
    "        reader.Update()\n",
    "\n",
    "        # 创建一个映射器\n",
    "        mapper = vtk.vtkPolyDataMapper()\n",
    "        mapper.SetInputConnection(reader.GetOutputPort())\n",
    "\n",
    "        # 创建一个演员\n",
    "        actor = vtk.vtkActor()\n",
    "        actor.SetMapper(mapper)\n",
    "\n",
    "        # 读取贴图文件\n",
    "        texture = vtk.vtkTexture()\n",
    "        texture_reader = vtk.vtkJPEGReader()  # 假设贴图为JPEG格式\n",
    "        texture_reader.SetFileName(pic_path)  # 根据.mtl文件中的路径设置\n",
    "        texture_reader.Update()\n",
    "        texture.SetInputConnection(texture_reader.GetOutputPort())\n",
    "        \n",
    "        # 将贴图应用到演员\n",
    "        actor.SetTexture(texture)\n",
    "        actor.SetOrientation(5,0,0)\n",
    "\n",
    "        # 创建渲染窗口\n",
    "        render_window = vtk.vtkRenderWindow()\n",
    "        render_window.SetSize(image_size[0], image_size[1])  # 设置渲染窗口大小 \n",
    "\n",
    "        # 创建左右视角的渲染器\n",
    "        total_pic_count = 36\n",
    "        renderers = []\n",
    "        for i in range(total_pic_count):\n",
    "            renderer = vtk.vtkRenderer()\n",
    "            render_window.AddRenderer(renderer)\n",
    "            renderers.append(renderer)\n",
    "            # 禁用光照\n",
    "            renderer.SetAmbient(100.0, 100.0, 100.0)  # 设置环境光\n",
    "            actor.GetProperty().SetLighting(False)\n",
    "            # 设置摄像机\n",
    "            camera = vtk.vtkCamera()\n",
    "            \n",
    "            camera.SetFocalPoint(0, 0, 0)\n",
    "            # if i == 0:  # 左视角\n",
    "            #     camera.SetPosition(-2, 0, 30)  # 可以根据需要调整位置\n",
    "            #     camera.SetViewUp(0, -1, 0)\n",
    "            # else:  # 右视角\n",
    "            #     camera.SetPosition(2, 0, 30)  # 右视角位置\n",
    "            #     camera.SetViewUp(0, -1, 0)\n",
    "\n",
    "      \n",
    "            camera.SetPosition(-9+i*0.5, 0, 22)  # 可以根据需要调整位置\n",
    "            camera.SetViewUp(0, -1, 0)\n",
    "\n",
    "            renderer.SetActiveCamera(camera)\n",
    "\n",
    "            # 添加演员到渲染器\n",
    "            renderer.AddActor(actor)\n",
    "            renderer.SetBackground(0.0, 0.0, 0.0)  # 背景颜色\n",
    "\n",
    "            # 渲染\n",
    "            render_window.Render()\n",
    "\n",
    "            # 保存图像\n",
    "            window_to_image_filter = vtk.vtkWindowToImageFilter()\n",
    "            window_to_image_filter.SetInput(render_window)\n",
    "            window_to_image_filter.ReadFrontBufferOff()  # 读取后台缓冲区\n",
    "            window_to_image_filter.Update()\n",
    "\n",
    "            writer = vtk.vtkPNGWriter()\n",
    "\n",
    "            #print(save_path.replace(\"_/\",f\"{i+1}/\"))\n",
    "            writer.SetFileName(save_path.replace(\"_/\",f\"{i+1}/\"))\n",
    "        \n",
    "            writer.SetInputConnection(window_to_image_filter.GetOutputPort())\n",
    "            writer.Write()\n",
    "\n",
    "    \n",
    "\n",
    "    render_and_save_images('a.obj',  'output.png')\n",
    "\n",
    "\n",
    "# pic_path = \"./37.jpg\"\n",
    "# save_path = \"./1.png\"\n",
    "# pic23d(pic_path,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 339)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def save_frames_from_video(video_path, output_folder, num_frames_range=[0,5]):\n",
    "    if  \".mp4\" in video_path:\n",
    "        # 创建输出文件夹\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # 打开视频文件\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # 检查视频是否成功打开\n",
    "        if not cap.isOpened():\n",
    "            print(\"无法打开视频文件\")\n",
    "            return\n",
    "\n",
    "        frame_count = 0\n",
    "\n",
    "\n",
    "        for i in range(0,36):\n",
    "            save_path = f\"{output_folder}/{i+1}/\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        #while (frame_count >= num_frames_range[0] and frame_count < num_frames_range[1]) :\n",
    "        # for i in range(num_frames_range[0],num_frames_range[1]):\n",
    "        pic_count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()  # 读取帧\n",
    "            if not ret:\n",
    "                break  # 如果没有更多帧，退出循环\n",
    "            frame_count += 1\n",
    "            if  (frame_count >= num_frames_range[0] and frame_count < num_frames_range[1]) :\n",
    "                pic_count+=1\n",
    "\n",
    "                # 保存帧为图像文件\n",
    "                frame_filename = os.path.join(output_folder, f'{pic_count}.jpg')\n",
    "                cv2.imwrite(frame_filename, frame)\n",
    "                \n",
    "\n",
    "                save_path = frame_filename.replace(\"blender_gif/\",\"blender_gif/_/\")\n",
    "                \n",
    "                #print(\">\",frame_filename)\n",
    "                pic23d(frame_filename,save_path)\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        cap.release()  # 释放视频捕获对象\n",
    "        print(f'已保存 {frame_count} 帧到 {output_folder}')\n",
    "    else:\n",
    "        for i in range(0,36):\n",
    "            save_path = f\"{output_folder}/{i+1}/\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        pic23d(video_path,video_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "# video_file = './video2.mp4'  # 视频文件路径\n",
    "\n",
    "video_file = 'result.jpg'  # 视频文件路径\n",
    "output_dir = './blender_gif_2/'    # 输出文件夹路径\n",
    "save_frames_from_video(video_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def upload_pic(pic_file_name, save_base=\"index.bin\"):\n",
    "    im = Image.open(pic_file_name)\n",
    "    \n",
    "    # Adjust the image size while maintaining the aspect ratio\n",
    "    original_width, original_height = im.size\n",
    "    target_height = 240\n",
    "    target_width = int((target_height / original_height) * original_width)\n",
    "    im = im.resize((target_width, target_height))  # Use LANCZOS filter for better quality\n",
    "\n",
    "    # Optionally, convert to RGB if the image has an alpha channel\n",
    "    if im.mode in (\"RGBA\", \"LA\") or (im.mode == \"P\" and \"transparency\" in im.info):\n",
    "        im = im.convert(\"RGB\")\n",
    "\n",
    "    # Save as a binary file (using JPEG with adjustable quality)\n",
    "    img_byte = BytesIO()\n",
    "    im.save(img_byte, format='JPEG', quality=100)  # Adjust quality as needed\n",
    "    img_data = img_byte.getvalue()\n",
    "\n",
    "    # Write to binary file\n",
    "    with open(save_base, \"wb\") as f:\n",
    "        f.write(img_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: 'C:/Users/Administrator/Pictures/blender_gif//120'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m target_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m33\u001b[39m,\u001b[38;5;241m121\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, pic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(   \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(pic_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)) ,key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]))        ) :\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pic \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pic:\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;66;03m# print(pic_path+pic)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:/3d/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: 'C:/Users/Administrator/Pictures/blender_gif//120'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pic_path = \"C:/Users/Administrator/Pictures/blender_gif/\"\n",
    "# pic_path = \"./blender_gif\"\n",
    "target_path = \"7\"\n",
    "for i in range(33,121):\n",
    "    for index, pic in enumerate(   sorted(os.listdir(pic_path+\"/\"+str(i)) ,key = lambda x: int(x.split(\".\")[0]))        ) :\n",
    "        if \".jpg\" in pic or \".png\" in pic:\n",
    "            # print(pic_path+pic)\n",
    "            if not os.path.exists(f\"F:/3d/{target_path}/\"+str(i)+\"/\"):\n",
    "                os.mkdir(f\"F:/3d/{target_path}/\"+str(i)+\"/\")\n",
    "            upload_pic(pic_path+\"/\"+str(i)+\"/\"+pic,f\"F:/3d/{target_path}/\"+str(i)+\"/\"+str(index)+\".bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'perspective2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mperspective2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PerspectiveFields \u001b[38;5;28;01mas\u001b[39;00m _PerspectiveFields\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'perspective2d'"
     ]
    }
   ],
   "source": [
    "from perspective2d import PerspectiveFields as _PerspectiveFields\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "from perspective2d import PerspectiveFields as _PerspectiveFields\n",
    "\n",
    "\n",
    "class PerspectiveFields(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Thin wrapper around perspective fields model for inferring camera intrinsics\n",
    "    \"\"\"\n",
    "    VERSIONS = {\n",
    "        \"Paramnet-360Cities-edina-centered\",\n",
    "        \"Paramnet-360Cities-edina-uncentered\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            version=\"Paramnet-360Cities-edina-uncentered\",\n",
    "            #\n",
    "            #Paramnet-360Cities-edina-centered\n",
    "            device=\"cuda\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            version (str): underlying model version to use. Valid options are:\n",
    "                {\"Paramnet-360Cities-edina-uncentered\", \"Paramnet-360Cities-edina-centered\"}\n",
    "            device (str): device to store tensors on. Default is \"cuda\"\n",
    "        \"\"\"\n",
    "        # Call super first\n",
    "        super().__init__()\n",
    "\n",
    "        # Sanity check version\n",
    "        assert version in self.VERSIONS,\\\n",
    "            f\"Got invalid PerspectiveFields version! Valid options: {self.VERSIONS}, got: {version}\"\n",
    "\n",
    "        # Load model\n",
    "        self.device = device\n",
    "        self.model = _PerspectiveFields(version)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def estimate_camera_intrinsics(self, input_path):\n",
    "        \"\"\"\n",
    "        Estimates the K camera intrinsics matrix using PerspectiveFields model\n",
    "\n",
    "        Args:\n",
    "            input_path (str): Absolute path to the image from which to infer camera intrinsics\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Estimated intrinsics matrix\n",
    "        \"\"\"\n",
    "        # Load the image\n",
    "        img_bgr = cv2.imread(input_path)\n",
    "\n",
    "        # Run predictions\n",
    "        predictions = self.model.inference(img_bgr=img_bgr)\n",
    "\n",
    "        # Compute intrinsics\n",
    "        height, width, _ = img_bgr.shape\n",
    "        # Convert vfov to radians\n",
    "        vfov_rad = math.radians(predictions['pred_general_vfov'].item())\n",
    "        # Compute focal length in pixels\n",
    "        f = (height / 2) / math.tan(vfov_rad / 2)\n",
    "        # Compute fx and fy\n",
    "        fx = f * predictions['pred_rel_focal'].item()\n",
    "        fy = f\n",
    "        # Compute cx and cy\n",
    "        cx = width / 2.0\n",
    "        cy = height / 2.0\n",
    "        # Construct the camera intrinsics matrix\n",
    "        K = torch.tensor([\n",
    "            [fx, 0, cx],\n",
    "            [0, fy, cy],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        print(K)\n",
    "\n",
    "        return K\n",
    "    \n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# import digital_cousins\n",
    "from metric_depth.depth_anything_v2.dpt import DepthAnythingV2 as _DepthAnythingV2\n",
    "\n",
    "# from digital_cousins.utils.processing_utils import process_depth_linear, unprocess_depth_linear\n",
    "\n",
    "def rescale_image(img, in_limits, out_limits):\n",
    "    \"\"\"\n",
    "    Rescales image from having values in range @in_limits to range @out_limits\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Image to be rescaled\n",
    "        in_limits (2-tuple): (min, max) range of the input image\n",
    "        out_limits (2-tuple): (min, max) range of the rescaled image\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Rescaled image\n",
    "    \"\"\"\n",
    "    # Out shape if specified should be (H, W) 2-tuple\n",
    "    # Keep absolute range to be from 0 to 10 --> renormalize to 0 - 1\n",
    "    in_min, in_max = in_limits\n",
    "    out_min, out_max = out_limits\n",
    "    scale_factor = abs(out_max - out_min) / abs(in_max - in_min)\n",
    "    out_tf = (out_max + out_min) / 2.0\n",
    "    in_tf = (in_max + in_min) / 2.0\n",
    "    img = (img - in_tf) * scale_factor + out_tf\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def process_depth_linear(depth, in_limits=(0, 10), out_limits=(0, 1), out_shape=None, use_16bit=True):\n",
    "    \"\"\"\n",
    "    Discretizes a linear depth map from range @in_limits to range @out_limits, and optionally resizes it to @out_shape\n",
    "\n",
    "    Args:\n",
    "        depth (np.ndarray): Input depth map with metric values\n",
    "        in_limits (2-tuple): (min, max) range of the input image\n",
    "        out_limits (2-tuple): (min, max) range of the processed image\n",
    "        out_shape (None or 2-tuple): If specified, the (H, W) @depth should be resized to\n",
    "        use_16bit (bool): Whether to use 16-bit or 8-bit uint encoding when rescaling the image\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Processed depth image.\n",
    "    \"\"\"\n",
    "    # Out shape if specified should be (H, W) 2-tuple\n",
    "    # Keep absolute range to be from 0 to 10 --> renormalize to 0 - 1\n",
    "    in_min, in_max = in_limits\n",
    "    foreground = np.where(depth <= in_max)\n",
    "    background = np.where(depth > in_max)\n",
    "    depth = rescale_image(img=depth, in_limits=in_limits, out_limits=out_limits)\n",
    "\n",
    "    # Zero out background\n",
    "    bit_size = 16 if use_16bit else 8\n",
    "    dtype = np.uint16 if use_16bit else np.uint8\n",
    "    depth[background] = 0.0\n",
    "    # Multiply by 2 ** 16\n",
    "    depth = depth * (2 ** bit_size)\n",
    "    depth = depth.astype(dtype)\n",
    "    if out_shape is not None:\n",
    "        depth = cv2.resize(depth, (out_shape[1], out_shape[0]))\n",
    "    return depth\n",
    "\n",
    "\n",
    "def unprocess_depth_linear(depth, in_limits=(0.0, 1.0), out_limits=(0.0, 10.0)):\n",
    "    \"\"\"\n",
    "    Unnormalizes a linear depth map from range @in_limits to range @out_limits. This process is the inverse\n",
    "    of @procedss_depth_linear)\n",
    "\n",
    "    Args:\n",
    "        depth (np.ndarray): Input depth map with normalized values (the output of @process_depth_linear)\n",
    "        in_limits (2-tuple): (min, max) range of the input image\n",
    "        out_limits (2-tuple): (min, max) range of the unprocessed image\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Unnormalized depth image.\n",
    "    \"\"\"\n",
    "    # Map to float, divide by 2**16, then invert transform\n",
    "    depth = depth.astype(float)\n",
    "    depth = depth / (2 ** 16)\n",
    "\n",
    "    # Keep unnormalize from 0 - 1 --> 0 - 10\n",
    "    in_min, in_max = in_limits\n",
    "    out_min, out_max = out_limits\n",
    "    scale_factor = abs(out_max - out_min) / abs(in_max - in_min)\n",
    "    out_tf = (out_max + out_min) / 2.0\n",
    "    in_tf = (in_max + in_min) / 2.0\n",
    "    depth = (depth - in_tf) * scale_factor + out_tf\n",
    "    return depth\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DepthAnythingV2(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Thin wrapper around DepthAnything V2 model for inferring depth maps\n",
    "    \"\"\"\n",
    "    # Maps backbone size to configuration to use\n",
    "    CONFIGS = {\n",
    "        'small': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "        'base': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "        'large': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    }\n",
    "\n",
    "    # Maps dataset name to max depth to use\n",
    "    DATASET_MAX_DEPTHS = {\n",
    "        \"hypersim\": 20,\n",
    "        \"vkitti\": 80,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone_size=\"base\",\n",
    "            backbone_dataset=\"hypersim\",\n",
    "            max_depth=None,\n",
    "            device=\"cuda\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            backbone_size (str): Size of underlying ViT model to use. Options are {'small', 'base', 'large'}\n",
    "            backbone_dataset (str): Which training dataset the underlying model was trained on.\n",
    "                Use \"hypersim\" for indoor estimation, \"vkitti\" for outdoor model\n",
    "            max_depth (None or int): Maximum depth (m) to use for depth estimation. If None, will use a default\n",
    "                based on @backbone_dataset\n",
    "            device (str): device to store tensors on. Default is \"cuda\"\n",
    "        \"\"\"\n",
    "        # Call super first\n",
    "        super().__init__()\n",
    "\n",
    "        # Sanity check values\n",
    "        assert backbone_size in self.CONFIGS,\\\n",
    "            f\"Got invalid DepthAnythingV2 backbone_size! Valid options: {self.CONFIGS.keys()}, got: {backbone_size}\"\n",
    "        assert backbone_dataset in self.DATASET_MAX_DEPTHS,\\\n",
    "            f\"Got invalid DepthAnythingV2 backbone_dataset! Valid options: {self.DATASET_MAX_DEPTHS.keys()}, got: {backbone_dataset}\"\n",
    "\n",
    "        self.max_depth = self.DATASET_MAX_DEPTHS[backbone_dataset] if max_depth is None else max_depth\n",
    "        self.device = 'cuda:0'\n",
    "\n",
    "        #Load model\n",
    "        self.model = _DepthAnythingV2(**{**self.CONFIGS[backbone_size], \"max_depth\": self.max_depth})\n",
    "        self.model.load_state_dict(\n",
    "            torch.load(\n",
    "                './checkpoints/depth_anything_v2_metric_hypersim_vitb.pth',\n",
    "                map_location=self.device,\n",
    "                weights_only=True,\n",
    "            ),\n",
    "        )\n",
    "        self.model.to(self.device).eval()\n",
    "\n",
    "        # self. model = _DepthAnythingV2(encoder='vitb', features=128, out_channels=[96, 192, 384, 768])\n",
    "        # self.model.load_state_dict(torch.load('./checkpoints/depth_anything_v2_vitb.pth', map_location='cuda:0')) #cpu\n",
    "        # self.model.cuda().eval() \n",
    "        #depth = model.infer_image(raw_img) # HxW raw depth map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def estimate_depth_linear(self, input_path, output_path, depth_limits=(0, 10.0)):\n",
    "        \"\"\"\n",
    "        Estimates linear depth map using DepthAnythingV2 model.\n",
    "\n",
    "        NOTE: depth linear implies that the outputted map is taken with respect to the image plane,\n",
    "            NOT the camera position!\n",
    "\n",
    "        Args:\n",
    "            input_path (str): Absolute path to the image from which to infer linear depth map\n",
    "            output_path (str): Absolute path to the location for the estimated linear depth map image\n",
    "            depth_limits (2-tuple): (min, max) values from the depth map to normalize scaling when saving the\n",
    "                depth map image\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Estimated linear depth map\n",
    "        \"\"\"\n",
    "        img = cv2.imread(input_path)\n",
    "        H, W, _ = img.shape\n",
    "        pred = self.model.infer_image(img)  # HxW depth map in meters in numpy\n",
    "\n",
    "        # Save output; normalize and convert the depth image to an 8-bit format if necessary\n",
    "        Path(os.path.dirname(output_path)).mkdir(parents=True, exist_ok=True)\n",
    "        depth = process_depth_linear(depth=pred, in_limits=depth_limits)\n",
    "        Image.fromarray(depth).save(output_path)\n",
    "\n",
    "        return depth\n",
    "\n",
    "    def crop_center(self, image, crop_percent=1):\n",
    "        \"\"\"\n",
    "        Crops the center of the image based on the specified crop percentage.\n",
    "        Works for both single-channel (grayscale/depth) and three-channel (RGB) images.\n",
    "\n",
    "        Params:\n",
    "            image (np.ndarray): A numpy array representing the image. Can be 2D (single-channel) or 3D (multi-channel).\n",
    "            crop_percent (int): The percentage of the image to retain in the center. Default is 0.8.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: A numpy array representing the cropped image.\n",
    "        \"\"\"\n",
    "\n",
    "        if crop_percent <= 0 or crop_percent > 1:\n",
    "            raise ValueError(\"crop_percent must be between 0 and 1\")\n",
    "\n",
    "        # Check the number of dimensions to handle both single and multi-channel images\n",
    "        if image.ndim == 3:  # Multi-channel image\n",
    "            height, width, _ = image.shape\n",
    "        elif image.ndim == 2:  # Single-channel image\n",
    "            height, width = image.shape\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image shape\")\n",
    "\n",
    "        # Calculate the new dimensions\n",
    "        new_height = int(height * crop_percent)\n",
    "        new_width = int(width * crop_percent)\n",
    "\n",
    "        # Calculate margins\n",
    "        top_margin = (height - new_height) // 2\n",
    "        left_margin = (width - new_width) // 2\n",
    "\n",
    "        # Crop the image based on the number of dimensions\n",
    "        if image.ndim == 3:\n",
    "            cropped_image = image[top_margin:top_margin + new_height, left_margin:left_margin + new_width, :]\n",
    "        else:  # image.ndim == 2\n",
    "            cropped_image = image[top_margin:top_margin + new_height, left_margin:left_margin + new_width]\n",
    "\n",
    "        print(f\"cropped_img_shape: {cropped_image.shape}\")\n",
    "        return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PerspectiveFields' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m camera_intrinsics_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m camera_intrinsics_matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     intrinsics_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mPerspectiveFields\u001b[49m(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     68\u001b[0m     intrinsics \u001b[38;5;241m=\u001b[39m intrinsics_estimator\u001b[38;5;241m.\u001b[39mestimate_camera_intrinsics(input_path\u001b[38;5;241m=\u001b[39minput_path)\n\u001b[0;32m     69\u001b[0m     intrinsics_estimator\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PerspectiveFields' is not defined"
     ]
    }
   ],
   "source": [
    "depth_max_limit = 20\n",
    "\n",
    "def compute_point_cloud_from_depth(depth, K, cam_to_img_tf=None, world_to_cam_tf=None, visualize_every=0,\n",
    "                                   grid_limits=None):\n",
    "    \"\"\"\n",
    "    Computes point cloud from depth image.\n",
    "\n",
    "    Args:\n",
    "        depth (np.ndarray): Input depth map with normalized values (the output of @process_depth_linear)\n",
    "        K (np.ndarray): 3x3 cam intrinsics matrix\n",
    "        cam_to_img_tf (np.ndarray): 4x4 Camera to image coordinate transformation matrix.\n",
    "                    omni cam_to_img_tf is T.pose2mat(([0, 0, 0], T.euler2quat([np.pi, 0, 0])))\n",
    "        world_to_cam_tf (np.ndarray): 4x4 World to camera coordinate transformation matrix.\n",
    "        visualize_every (int): Step size when uniformly sampling points in the resulting point cloud to visualize.\n",
    "        grid_limits (float): Visualization plot grid limits.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Resulting point cloud.\n",
    "    \"\"\"\n",
    "    \n",
    "    h, w = depth.shape\n",
    "    y, x = np.meshgrid(np.arange(h), np.arange(w), indexing=\"ij\", sparse=False)\n",
    "    assert depth.min() >= 0\n",
    "    u = x\n",
    "    v = y\n",
    "    uv = np.dstack((u, v, np.ones_like(u)))\n",
    "\n",
    "    Kinv = np.linalg.inv(K)\n",
    "\n",
    "    pc = depth.reshape(-1, 1) * (uv.reshape(-1, 3) @ Kinv.T)\n",
    "    pc = pc.reshape(h, w, 3)\n",
    "\n",
    "    # import open3d as o3d\n",
    "    # pcd = o3d.geometry.PointCloud()\n",
    "    # pcd.points = o3d.utility.Vector3dVector(pc.reshape(-1, 3))\n",
    "    # # pcd.colors = o3d.utility.Vector3dVector(rgb.reshape(-1, 3) / 255.0)\n",
    "    # o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    # If no tfs, use identity matrix\n",
    "    cam_to_img_tf = np.eye(4) if cam_to_img_tf is None else cam_to_img_tf\n",
    "    world_to_cam_tf = np.eye(4) if world_to_cam_tf is None else world_to_cam_tf\n",
    "\n",
    "    pc = np.concatenate([pc.reshape(-1, 3), np.ones((h * w, 1))], axis=-1)  # shape (H*W, 4)\n",
    "\n",
    "    # Convert using camera transform\n",
    "    # Create (H * W, 4) vector from pc\n",
    "    pc = (pc @ cam_to_img_tf.T @ world_to_cam_tf.T)[:, :3].reshape(h, w, 3)\n",
    "    return pc,h,w\n",
    "device = \"cuda:0\"\n",
    "input_path = \"./Q8.jpg\"\n",
    "save_dir = \".\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "raw_img = cv2.imread(input_path)  \n",
    "height, width = raw_img.shape[:2]  \n",
    "target_height = 600  \n",
    "scale_ratio = target_height / height  \n",
    "target_width = int(width * scale_ratio)  \n",
    "raw_img = cv2.resize(raw_img, (target_width, target_height))  \n",
    "cv2.imwrite(input_path, raw_img)\n",
    "\n",
    "camera_intrinsics_matrix = None\n",
    "if camera_intrinsics_matrix is None:\n",
    "   \n",
    "    intrinsics_estimator = PerspectiveFields(device=device)\n",
    "    intrinsics = intrinsics_estimator.estimate_camera_intrinsics(input_path=input_path)\n",
    "    intrinsics_estimator.to('cpu')\n",
    "    del intrinsics_estimator\n",
    "    camera_intrinsics_matrix = np.array(intrinsics)\n",
    "\n",
    "\n",
    "depth_path = f\"{save_dir}/step_1_depth.png\"\n",
    "depth_limits = np.array([0, depth_max_limit])\n",
    "depth_estimator = DepthAnythingV2(device=device)\n",
    "\n",
    "\n",
    "\n",
    "depth_estimator.estimate_depth_linear(input_path=input_path, output_path=depth_path, depth_limits=depth_limits)\n",
    "depth_estimator.to('cpu')\n",
    "del depth_estimator\n",
    "\n",
    "depth = unprocess_depth_linear(np.array(Image.open(depth_path)), out_limits=depth_limits)\n",
    "pc = compute_point_cloud_from_depth(depth=depth, K=camera_intrinsics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "visualize = True\n",
    "if visualize:\n",
    "    # Explicitly manage visualization process to prevent conflict with og launching process\n",
    "    def vis():\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(pc[0].reshape(-1, 3))\n",
    "        # pcd.colors = o3d.utility.Vector3dVector(rgb.reshape(-1, 3) / 255.0)\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "    # vis_process = multiprocessing.Process(target=vis)\n",
    "    # vis_process.start()\n",
    "    # vis_process.join()\n",
    "vis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v -5.718049773986422 -8.629110509991733 19.99969482421875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in pc[0]:\n",
    "    print( \"v \"+str(l[0][0])+\" \"+str(l[0][1])+\" \"+str(l[0][2])+\"\\n\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270000, 3)\n",
      "(600, 450)\n"
     ]
    }
   ],
   "source": [
    "pos ,height ,width= pc  #pc,h,w\n",
    "pos = pos.reshape(-1, 3)\n",
    "print(pos.shape)\n",
    "with open(\"a.obj\",\"w\") as t:\n",
    "    t.writelines(\"mtllib my_mtl.mtl\"+\"\\n\")\n",
    "        \n",
    "    for l in pos:\n",
    "        L =  \"v \"+str(l[0])+\" \"+str(l[1])+\" \"+str(l[2])+\"\\n\"\n",
    "        t.writelines(L )\n",
    "    \n",
    "    w_x = np.asarray([np.linspace(0.0, 1.0, num=width)]*height).reshape(width*height)\n",
    "    h_y = np.asarray([np.linspace(1.0, 0.0, num=height)]*width).T.reshape(width*height)\n",
    "    t_array_ = np.stack([w_x,h_y],axis=1)\n",
    "\n",
    "\n",
    "    for i,j in t_array_:\n",
    "        L = \"vt \" +str(i)+\" \"+str(j) +\"\\n\"\n",
    "        t.writelines(L )                \n",
    "#  \n",
    "#          \n",
    "    t.writelines(\"\\nusemtl my_mtl\\n\" )              \n",
    "    b = np.array(range(height * width)).reshape([ height ,width])\n",
    "    print(b.shape)  \n",
    "            \n",
    "    for i in range(1,b.shape[0]-1):\n",
    "        for j in range(1,b.shape[1]-1):\n",
    "            p = [ b[i,j] , b[i+1,j],b[i+1,j+1],b[i,j+1]]\n",
    "    \n",
    "            L = \"f \"+str(p[0])+\"/\"+ str(p[0])+\" \"+str(p[1])+\"/\"+ str(p[1])+\" \"+str(p[2])+\"/\"+ str(p[2])+\" \"+str(p[3])+\"/\"+ str(p[3])+\"\\n\"\n",
    "            t.writelines(L )\n",
    "        \n",
    "\n",
    "\n",
    "with open(\"my_mtl.mtl\",\"w\") as m:  \n",
    "    m.writelines(\n",
    "f\"\"\"\n",
    "newmtl my_mtl\n",
    "Ka 1 1 1\n",
    "Kd 1 1 1\n",
    "d 1\n",
    "Ns 0\n",
    "illum 1\n",
    "map_Kd {input_path}\n",
    "\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************\n",
      "Starting tests...\n",
      "******************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vv\\Miniconda3\\envs\\llama2\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      ".//checkpoints\n",
      "['depth_anything_v2_metric_hypersim_vitb.pth', 'depth_anything_v2_vitb.pth', 'depth_anything_v2_vits.pth']\n"
     ]
    }
   ],
   "source": [
    "import digital_cousins\n",
    "# If you store the offline dataset elsewhere, please uncomment the following line and put the directory here\n",
    "# digital_cousins.ASSET_DIR = \"~/assets\"\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "# import digital_cousins\n",
    "\n",
    "TEST_DIR = os.path.dirname(\"test_img.png\")\n",
    "SAVE_DIR = f\"{TEST_DIR}/test_acdc_output\"\n",
    "TEST_IMG_PATH = f\"{TEST_DIR}/test_img.png\"\n",
    "CAPTION = \"Fridge. Cabinet.\"\n",
    "\n",
    "\n",
    "def test_perspective_fields(TEST_IMG_PATH):\n",
    "    from digital_cousins.models.perspective_fields import PerspectiveFields\n",
    "    PerspectiveFields().estimate_camera_intrinsics(input_path=TEST_IMG_PATH)\n",
    "\n",
    "\n",
    "def test_depth_anything_2(TEST_IMG_PATH):\n",
    "    from digital_cousins.models.depth_anything_v2 import DepthAnythingV2\n",
    "    depth_path = f\"test_depth.png\"\n",
    "    DepthAnythingV2().estimate_depth_linear(input_path=TEST_IMG_PATH, output_path=depth_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Run all tests\n",
    "    print()\n",
    "    print(\"*\" * 30)\n",
    "    print(\"Starting tests...\")\n",
    "    print(\"*\" * 30)\n",
    "    print()\n",
    "    TEST_IMG_PATH = \"./test_img.png\"\n",
    "  \n",
    "    test_perspective_fields(TEST_IMG_PATH)\n",
    "    test_depth_anything_2(TEST_IMG_PATH)\n",
    " \n",
    "main()\n",
    "\n",
    "    # Final test -- OG should always come at the end\n",
    "    # This og test cannot run together with test_acdc_step_3\n",
    "    # because the simulator can only be launched once, and after calling og.shutdown(), the whole process will terminate\n",
    "    # test_og(args)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define args\n",
    "#     parser = argparse.ArgumentParser()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpy\n",
    "import os\n",
    "\n",
    "# 设置保存路径\n",
    "\n",
    "\n",
    "\n",
    "# 获取场景中的Cube对象\n",
    "cube = bpy.data.objects.get(\"圆环\")\n",
    "if not cube:\n",
    "    raise ValueError(\"找不到名为 'Cube' 的对象，请确保它存在\")\n",
    "\n",
    "# 配置渲染设置\n",
    "bpy.context.scene.render.image_settings.file_format = 'PNG'  # 渲染为PNG格式\n",
    "bpy.context.scene.render.resolution_x = 800  # 宽度\n",
    "bpy.context.scene.render.resolution_y = 800  # 高度\n",
    "\n",
    "# 旋转和渲染逻辑\n",
    "for index, i in enumerate( list( range(-180, 180, 5))):\n",
    "    \n",
    "    save_path = f\"C:/Users/Administrator/Pictures/blender_gif/{index}/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # 旋转Cube对象\n",
    "    cube.rotation_euler[2] = i * (3.14159 / 180)  # 将角度转换为弧度\n",
    "    \n",
    "    for frame in range(1,43):\n",
    "    \n",
    "        # 设置帧号（用于保存文件名）\n",
    "        bpy.context.scene.frame_set(frame)  # 每5度为一帧\n",
    "    \n",
    "        # 设置输出文件路径\n",
    "        file_name = f\"{frame}.png\"\n",
    "        bpy.context.scene.render.filepath = os.path.join(save_path, file_name)\n",
    "    \n",
    "        # 渲染当前帧\n",
    "        #bpy.ops.render.render(write_still=True)\n",
    "        bpy.ops.render.opengl(write_still=True, view_context=True)\n",
    "    index+=1\n",
    "  \n",
    "\n",
    "print(\"渲染完成！所有图片保存在：\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.outputs import OutputKeys\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 3 clips.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def split_video_by_frames(input_file, frames_per_clip, output_dir='./'):\n",
    "    # 创建输出目录如果不存在\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 打开视频文件\n",
    "    cap = cv2.VideoCapture(input_file)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_file}\")\n",
    "        return\n",
    "    \n",
    "    # 获取视频的帧率、帧大小和四字符代码（Four Character Code, FCC）\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 或者使用cv2.VideoWriter_fourcc('X','V','I','D')等\n",
    "    \n",
    "    clip_index = 0\n",
    "    frame_count = 0\n",
    "    out = None\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # 视频读取结束\n",
    "        \n",
    "        if frame_count % frames_per_clip == 0:\n",
    "            # 如果需要，关闭上一个视频写入器\n",
    "            if out is not None:\n",
    "                out.release()\n",
    "            \n",
    "            # 创建新的视频写入器\n",
    "            clip_filename = os.path.join(output_dir, f\"clip_{clip_index:03d}.mp4\")\n",
    "            out = cv2.VideoWriter(clip_filename, fourcc, fps, (width, height))\n",
    "            clip_index += 1\n",
    "        \n",
    "        # 写入当前帧到视频文件\n",
    "        out.write(frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    # 释放视频捕获和视频写入器对象\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "    \n",
    "    print(f\"Split into {clip_index} clips.\")\n",
    "\n",
    "# 使用示例\n",
    "input_file = \"b3b.mp4\"\n",
    "frames_per_clip = 100\n",
    "split_video_by_frames(input_file, frames_per_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from moviepy.editor import ImageSequenceClip, AudioFileClip\n",
    "\n",
    "def extract_frames(video_path, output_folder):\n",
    "    # 打开视频文件\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    \n",
    "    # 确保输出文件夹存在\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    while success:\n",
    "        # 保存帧为图片文件\n",
    "        cv2.imwrite(os.path.join(output_folder, f\"frame_{count:04d}.jpg\"), image)\n",
    "        success, image = vidcap.read()\n",
    "        count += 1\n",
    "    \n",
    "    vidcap.release()\n",
    "    print(f\"Extracted {count} frames.\")\n",
    "\n",
    "def combine_frames_to_video(image_folder, output_video_path, audio_path, fps=30):\n",
    "    # 读取图片文件并排序\n",
    "    image_files = sorted([os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith(\".jpg\")])\n",
    "    \n",
    "    # 创建图片序列剪辑\n",
    "    clip = ImageSequenceClip(image_files, fps=fps)\n",
    "    \n",
    "    # 添加音频\n",
    "    audio = AudioFileClip(audio_path)\n",
    "    clip = clip.set_audio(audio)\n",
    "    \n",
    "    # 写入输出视频文件\n",
    "    clip.write_videofile(output_video_path, codec='libx264', audio_codec='aac')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_video_path = \"input_video.mp4\"\n",
    "    output_image_folder = \"extracted_frames\"\n",
    "    output_video_path = \"output_video.mp4\"\n",
    "    \n",
    "    # 提取帧\n",
    "    extract_frames(input_video_path, output_image_folder)\n",
    "    \n",
    "    # 获取输入视频的音频文件\n",
    "    input_video_audio_path = input_video_path\n",
    "    \n",
    "    # 合并帧和音频成新的视频\n",
    "    combine_frames_to_video(output_image_folder, output_video_path, input_video_audio_path)\n",
    "    \n",
    "    print(\"Processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 19:34:27,979 - modelscope - WARNING - Authentication has expired, please re-login with modelscope login --token \"YOUR_SDK_TOKEN\" if you need to access private models or datasets.\n",
      "2024-12-05 19:34:29,436 - modelscope - WARNING - Model revision not specified, use revision: v1.0.1\n",
      "2024-12-05 19:34:30,030 - modelscope - INFO - initiate model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\cv_realbasicvsr_video-super-resolution_videolq\n",
      "2024-12-05 19:34:30,031 - modelscope - INFO - initiate model from location C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\cv_realbasicvsr_video-super-resolution_videolq.\n",
      "2024-12-05 19:34:30,032 - modelscope - INFO - initialize model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\cv_realbasicvsr_video-super-resolution_videolq\n",
      "2024-12-05 19:34:30,081 - modelscope - INFO - Loading RealBasicVSRNet model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\cv_realbasicvsr_video-super-resolution_videolq\\pytorch_model.pt, with param key: [params].\n",
      "2024-12-05 19:34:30,109 - modelscope - INFO - load model done.\n",
      "2024-12-05 19:34:30,112 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-12-05 19:34:30,113 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-12-05 19:34:30,113 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\cv_realbasicvsr_video-super-resolution_videolq'}. trying to build by task and model information.\n",
      "2024-12-05 19:34:30,113 - modelscope - WARNING - No preprocessor key ('real-basicvsr', 'video-super-resolution') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
      "2024-12-05 19:34:30,124 - modelscope - INFO - load video super-resolution model done\n",
      "2024-12-05 19:34:30,245 - modelscope - WARNING - task video-super-resolution input definition is missing\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.27 GiB. GPU 0 has a total capacty of 11.99 GiB of which 0 bytes is free. Of the allocated memory 9.86 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./clip_000.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m video_super_resolution_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m      3\u001b[0m     Tasks\u001b[38;5;241m.\u001b[39mvideo_super_resolution,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdamo/cv_realbasicvsr_video-super-resolution_videolq\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_super_resolution_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m[OutputKeys\u001b[38;5;241m.\u001b[39mOUTPUT_VIDEO]\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\modelscope\\pipelines\\base.py:220\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_iterator(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\modelscope\\pipelines\\base.py:255\u001b[0m, in \u001b[0;36mPipeline._process_single\u001b[1;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_collate:\n\u001b[0;32m    254\u001b[0m             out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collate_fn(out)\n\u001b[1;32m--> 255\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\modelscope\\pipelines\\cv\\video_super_resolution_pipeline.py:137\u001b[0m, in \u001b[0;36mVideoSuperResolutionPipeline.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    133\u001b[0m         imgs \u001b[38;5;241m=\u001b[39m inputs[:,\n\u001b[0;32m    134\u001b[0m                       i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmax_seq_len, :, :, :]\n\u001b[0;32m    135\u001b[0m         imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\n\u001b[0;32m    136\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 137\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m    138\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\modelscope\\models\\cv\\video_super_resolution\\real_basicvsr_for_video_super_resolution.py:60\u001b[0m, in \u001b[0;36mRealBasicVSRNetForVideoSR._inference_forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inference_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)}\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\modelscope\\models\\cv\\video_super_resolution\\real_basicvsr_net.py:76\u001b[0m, in \u001b[0;36mRealBasicVSRNet.forward\u001b[1;34m(self, lqs, return_lqs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# time -> batch, then apply cleaning at once\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     lqs \u001b[38;5;241m=\u001b[39m lqs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, c, h, w)\n\u001b[1;32m---> 76\u001b[0m     residues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_cleaning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     lqs \u001b[38;5;241m=\u001b[39m (lqs \u001b[38;5;241m+\u001b[39m residues)\u001b[38;5;241m.\u001b[39mview(n, t, c, h, w)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# determine whether to continue cleaning\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\modelscope\\models\\cv\\video_super_resolution\\basicvsr_net.py:223\u001b[0m, in \u001b[0;36mResidualBlocksWithInputConv.forward\u001b[1;34m(self, feat)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, feat):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward function for ResidualBlocksWithInputConv.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m        feat (Tensor): Input feature with shape (n, in_channels, h, w)\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m        Tensor: Output feature with shape (n, out_channels, h, w)\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\unique3d\\Lib\\site-packages\\modelscope\\models\\cv\\video_super_resolution\\common.py:41\u001b[0m, in \u001b[0;36mResidualBlockNoBN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     40\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43midentity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres_scale\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.27 GiB. GPU 0 has a total capacty of 11.99 GiB of which 0 bytes is free. Of the allocated memory 9.86 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "video = './clip_000.mp4'\n",
    "video_super_resolution_pipeline = pipeline(\n",
    "    Tasks.video_super_resolution,\n",
    "    'damo/cv_realbasicvsr_video-super-resolution_videolq')\n",
    "result = video_super_resolution_pipeline(video)[OutputKeys.OUTPUT_VIDEO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:39:34,778 - modelscope - WARNING - Authentication has expired, please re-login with modelscope login --token \"YOUR_SDK_TOKEN\" if you need to access private models or datasets.\n",
      "2024-12-05 22:39:36,575 - modelscope - WARNING - Model revision not specified, use revision: v1.1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad475ccae536478290be5b89c35a446f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [controlnet/diffusion_pytorch_model.safetensors]:   0%|          | 0.00/1.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:43:39,606 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\controlnet/diffusion_pytorch_model.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 22:43:39,635 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\controlnet/diffusion_pytorch_model.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 22:45:36,099 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\controlnet/diffusion_pytorch_model.safetensors failed, reason: ('Connection broken: IncompleteRead(56006217 bytes read, 111765943 more expected)', IncompleteRead(56006217 bytes read, 111765943 more expected)) will retry\n",
      "2024-12-05 22:48:39,787 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\controlnet/diffusion_pytorch_model.safetensors failed, reason: ('Connection broken: IncompleteRead(36970472 bytes read, 130801688 more expected)', IncompleteRead(36970472 bytes read, 130801688 more expected)) will retry\n",
      "2024-12-05 22:48:41,081 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\controlnet/diffusion_pytorch_model.safetensors failed, reason: ('Connection broken: IncompleteRead(41440180 bytes read, 70757452 more expected)', IncompleteRead(41440180 bytes read, 70757452 more expected)) will retry\n",
      "2024-12-05 22:48:41,511 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\controlnet/diffusion_pytorch_model.safetensors failed, reason: ('Connection broken: IncompleteRead(75877421 bytes read, 91894739 more expected)', IncompleteRead(75877421 bytes read, 91894739 more expected)) will retry\n",
      "2024-12-05 22:49:53,784 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\controlnet/diffusion_pytorch_model.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0dee89389724c6e80924c6b5c053afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer/merges.txt]:   0%|          | 0.00/512k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57264f6e572140348f9777a8626e0387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model_index.json]:   0%|          | 0.00/543 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7f54630ca140ff8c82fc73496c2aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [pictures/pasd_arch.png]:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a417c34cc9e4b85a031a516981a6922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [feature_extractor/preprocessor_config.json]:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f88712be00c49018d88e6ca8613069c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [text_encoder/pytorch_model.bin]:   0%|          | 0.00/469M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9629774107484eb5287a52fea131bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [text_encoder/pytorch_model.bin]:   0%|          | 0.00/469M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc25c314f1845c3af4737471b9bbceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [text_encoder/pytorch_model.bin]:   0%|          | 0.00/469M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06234d94332d4f4683090355779c7804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/4.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc74cdfb68e04af788d5bd9879326474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [realistic.safetensors]:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 23:07:56,271 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:07:56,301 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:07:56,316 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:07:57,844 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:12:10,550 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: ('Connection broken: IncompleteRead(51588511 bytes read, 92066401 more expected)', IncompleteRead(51588511 bytes read, 92066401 more expected)) will retry\n",
      "2024-12-05 23:12:59,560 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:13:06,428 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:13:11,445 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:13:30,536 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:13:55,794 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: ('Connection broken: IncompleteRead(445856 bytes read, 55128672 more expected)', IncompleteRead(445856 bytes read, 55128672 more expected)) will retry\n",
      "2024-12-05 23:14:39,358 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:17:25,970 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:26:00,028 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:27:17,877 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:31:10,519 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. will retry\n",
      "2024-12-05 23:31:28,597 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: ('Connection broken: IncompleteRead(50921309 bytes read, 116850851 more expected)', IncompleteRead(50921309 bytes read, 116850851 more expected)) will retry\n",
      "2024-12-05 23:31:34,605 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: ('Connection broken: IncompleteRead(45158602 bytes read, 95350582 more expected)', IncompleteRead(45158602 bytes read, 95350582 more expected)) will retry\n",
      "2024-12-05 23:34:33,326 - modelscope - WARNING - Downloading: C:\\Users\\Administrator\\.cache\\modelscope\\hub\\._____temp\\damo\\PASD_v2_image_super_resolutions\\realistic.safetensors failed, reason: ('Connection broken: IncompleteRead(152675677 bytes read, 15096483 more expected)', IncompleteRead(152675677 bytes read, 15096483 more expected)) will retry\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6055928c7541b788e337d3af4c71f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [requirements.txt]:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f047c65973fc49219d8b44fb3c21a6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [RetinaFace-R50.pth]:   0%|          | 0.00/104M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c260e1c7d4d4ea8ab121862dff11956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [scheduler/scheduler_config.json]:   0%|          | 0.00/308 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee76d83d9a3c4b63ad48f54b8f11a573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer/special_tokens_map.json]:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3004a235149c4155bfac110b14d88eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer/tokenizer_config.json]:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c48d4306e524aefa57c443525b261e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer/vocab.json]:   0%|          | 0.00/1.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 23:43:08,200 - modelscope - INFO - initiate model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\PASD_v2_image_super_resolutions\n",
      "2024-12-05 23:43:08,201 - modelscope - INFO - initiate model from location C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\PASD_v2_image_super_resolutions.\n",
      "2024-12-05 23:43:08,206 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-12-05 23:43:08,207 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-12-05 23:43:08,207 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\PASD_v2_image_super_resolutions'}. trying to build by task and model information.\n",
      "2024-12-05 23:43:08,208 - modelscope - WARNING - Find task: image-super-resolution-pasd, model type: None. Insufficient information to build preprocessor, skip building preprocessor\n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\diffusers\\models\\dual_transformer_2d.py:20: FutureWarning: `DualTransformer2DModel` is deprecated and will be removed in version 0.29. Importing `DualTransformer2DModel` from `diffusers.models.dual_transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.dual_transformer_2d import DualTransformer2DModel`, instead.\n",
      "  deprecate(\"DualTransformer2DModel\", \"0.29\", deprecation_message)\n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\diffusers\\models\\transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"0.29\", deprecation_message)\n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\diffusers\\models\\transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.\n",
      "  deprecate(\"Transformer2DModel\", \"0.29\", deprecation_message)\n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\Administrator/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [06:38<00:00, 257kB/s] \n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcc2eac44fc48e596e5bfa1008260f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tiled VAE]: the input size is tiny and unnecessary to tile.\n",
      "pipeline: the output image path is result.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from modelscope.outputs import OutputKeys\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "input_location = 'https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/resources/woman.png'\n",
    "prompt = ''\n",
    "output_image_path = 'result.png'\n",
    "\n",
    "input = {\n",
    "    'image': input_location,\n",
    "    'prompt': prompt,\n",
    "    'upscale': 2,\n",
    "    'fidelity_scale_fg': 1.0,\n",
    "    'fidelity_scale_bg': 1.0\n",
    "}\n",
    "pasd = pipeline(Tasks.image_super_resolution_pasd, model='damo/PASD_v2_image_super_resolutions', version='pasd_v2')\n",
    "output = pasd(input)[OutputKeys.OUTPUT_IMG]\n",
    "cv2.imwrite(output_image_path, output)\n",
    "print('pipeline: the output image path is {}'.format(output_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 02:42:32,047 - modelscope - WARNING - Authentication has expired, please re-login with modelscope login --token \"YOUR_SDK_TOKEN\" if you need to access private models or datasets.\n",
      "2024-12-06 02:42:33,540 - modelscope - WARNING - Model revision not specified, use revision: v1.1.0\n",
      "2024-12-06 02:42:34,646 - modelscope - INFO - initiate model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\PASD_v2_image_super_resolutions\n",
      "2024-12-06 02:42:34,647 - modelscope - INFO - initiate model from location C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\PASD_v2_image_super_resolutions.\n",
      "2024-12-06 02:42:34,653 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-12-06 02:42:34,655 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-12-06 02:42:34,655 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\PASD_v2_image_super_resolutions'}. trying to build by task and model information.\n",
      "2024-12-06 02:42:34,656 - modelscope - WARNING - Find task: image-super-resolution-pasd, model type: None. Insufficient information to build preprocessor, skip building preprocessor\n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\diffusers\\models\\dual_transformer_2d.py:20: FutureWarning: `DualTransformer2DModel` is deprecated and will be removed in version 0.29. Importing `DualTransformer2DModel` from `diffusers.models.dual_transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.dual_transformer_2d import DualTransformer2DModel`, instead.\n",
      "  deprecate(\"DualTransformer2DModel\", \"0.29\", deprecation_message)\n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\diffusers\\models\\transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"0.29\", deprecation_message)\n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\diffusers\\models\\transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.\n",
      "  deprecate(\"Transformer2DModel\", \"0.29\", deprecation_message)\n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\conda\\envs\\unique3d\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from modelscope.outputs import OutputKeys\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "pasd = pipeline(Tasks.image_super_resolution_pasd, model='damo/PASD_v2_image_super_resolutions', version='pasd_v2')\n",
    "\n",
    "\n",
    "def process_pic(input_location, output_image_path = 'result.png'):\n",
    "    prompt = 'Nude Girl,masterpiece, highly detailed'\n",
    "   \n",
    "    input = {\n",
    "        'image': input_location,\n",
    "        'prompt': prompt,\n",
    "        'upscale': 2,\n",
    "        'fidelity_scale_fg': 1.0,\n",
    "        'fidelity_scale_bg': 1.0\n",
    "    }\n",
    "    \n",
    "    output = pasd(input)[OutputKeys.OUTPUT_IMG]\n",
    "    cv2.imwrite(output_image_path, output)\n",
    "    print('pipeline: the output image path is {}'.format(output_image_path))\n",
    "# input_location = '215756.png'\n",
    "# process_pic(input_location, output_image_path = 'result.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224354424c0e4b8ebda8315c28e762f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tiled VAE]: the input size is tiny and unnecessary to tile.\n",
      "pipeline: the output image path is result.jpg\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "input_location = '38.jpg'\n",
    "\n",
    "from PIL import Image\n",
    "im = Image.open(input_location)\n",
    "\n",
    "# Adjust the image size while maintaining the aspect ratio\n",
    "original_width, original_height = im.size\n",
    "target_height = 500\n",
    "target_width = int((target_height / original_height) * original_width)\n",
    "im = im.resize((target_width, target_height))  # Use LANCZOS filter for better quality\n",
    "\n",
    "# Optionally, convert to RGB if the image has an alpha channel\n",
    "if im.mode in (\"RGBA\", \"LA\") or (im.mode == \"P\" and \"transparency\" in im.info):\n",
    "    im = im.convert(\"RGB\")\n",
    "\n",
    "# Save as a binary file (using JPEG with adjustable quality)\n",
    "# img_byte = BytesIO()\n",
    "im.save(input_location, format='JPEG', quality=100)  # Adjust quality as needed\n",
    "\n",
    "\n",
    "\n",
    "process_pic(input_location, output_image_path = 'result.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
